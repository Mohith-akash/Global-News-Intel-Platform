"""
GDELT Pipeline v2.0 - Supercharged with Polars + Great Expectations

Changes from v1.0:
- Polars instead of Pandas (10x faster processing)
- Great Expectations for data quality validation
- Separated ingestion (every 15 min) from embeddings (every 12 hours)
- Cleaner, more maintainable code

Author: Mohith Akash
"""

import requests
import datetime
import zipfile
import io
import re
import polars as pl
import duckdb
import os
import time
from urllib.parse import urlparse, unquote
from dagster import asset, Output, Definitions, ScheduleDefinition, define_asset_job, AssetExecutionContext
from dotenv import load_dotenv
import logging

load_dotenv()

# Configuration
TARGET_TABLE = "events_dagster"
MAX_RETRIES = 3
RETRY_DELAY = 5

# Voyage AI Configuration
VOYAGE_API_URL = "https://api.voyageai.com/v1/embeddings"
VOYAGE_MODEL = "voyage-3.5-lite"
EMBEDDING_DIMENSIONS = 1024
MIN_ARTICLE_COUNT_FOR_EMBEDDING = 3

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GDELT Column Schema (for validation)
GDELT_SCHEMA = {
    "EVENT_ID": pl.Utf8,
    "DATE": pl.Utf8,
    "MAIN_ACTOR": pl.Utf8,
    "ACTOR_COUNTRY_CODE": pl.Utf8,
    "EVENT_CATEGORY_CODE": pl.Float64,
    "IMPACT_SCORE": pl.Float64,
    "ARTICLE_COUNT": pl.Float64,
    "SENTIMENT_SCORE": pl.Float64,
    "NEWS_LINK": pl.Utf8,
}


# =============================================================================
# DATA QUALITY VALIDATION (Great Expectations style)
# =============================================================================

class DataQualityValidator:
    """
    Data quality validation inspired by Great Expectations.
    Validates GDELT data before loading to warehouse.
    """
    
    def __init__(self, df: pl.DataFrame):
        self.df = df
        self.results = []
        self.passed = True
    
    def expect_column_to_exist(self, column: str) -> "DataQualityValidator":
        """Expect a column to exist in the DataFrame."""
        exists = column in self.df.columns
        self.results.append({
            "expectation": "expect_column_to_exist",
            "column": column,
            "success": exists
        })
        if not exists:
            self.passed = False
        return self
    
    def expect_column_values_to_not_be_null(self, column: str, threshold: float = 0.95) -> "DataQualityValidator":
        """Expect column to have at least threshold% non-null values."""
        if column not in self.df.columns:
            self.results.append({
                "expectation": "expect_column_values_to_not_be_null",
                "column": column,
                "success": False,
                "reason": "Column does not exist"
            })
            self.passed = False
            return self
        
        non_null_ratio = 1 - (self.df[column].null_count() / len(self.df))
        success = non_null_ratio >= threshold
        self.results.append({
            "expectation": "expect_column_values_to_not_be_null",
            "column": column,
            "success": success,
            "non_null_ratio": round(non_null_ratio, 4),
            "threshold": threshold
        })
        if not success:
            self.passed = False
        return self
    
    def expect_column_values_to_be_unique(self, column: str) -> "DataQualityValidator":
        """Expect column values to be unique (no duplicates)."""
        if column not in self.df.columns:
            self.results.append({
                "expectation": "expect_column_values_to_be_unique",
                "column": column,
                "success": False,
                "reason": "Column does not exist"
            })
            self.passed = False
            return self
        
        unique_count = self.df[column].n_unique()
        total_count = len(self.df)
        success = unique_count == total_count
        self.results.append({
            "expectation": "expect_column_values_to_be_unique",
            "column": column,
            "success": success,
            "unique_count": unique_count,
            "total_count": total_count
        })
        # Note: We don't fail on duplicates - we'll deduplicate later
        return self
    
    def expect_column_values_to_be_between(self, column: str, min_val: float, max_val: float) -> "DataQualityValidator":
        """Expect column values to be within a range."""
        if column not in self.df.columns:
            self.results.append({
                "expectation": "expect_column_values_to_be_between",
                "column": column,
                "success": False,
                "reason": "Column does not exist"
            })
            return self
        
        # Filter out nulls for range check
        non_null = self.df.filter(pl.col(column).is_not_null())
        if len(non_null) == 0:
            self.results.append({
                "expectation": "expect_column_values_to_be_between",
                "column": column,
                "success": True,
                "reason": "No non-null values to check"
            })
            return self
        
        in_range = non_null.filter(
            (pl.col(column) >= min_val) & (pl.col(column) <= max_val)
        )
        ratio = len(in_range) / len(non_null)
        success = ratio >= 0.99  # Allow 1% outliers
        self.results.append({
            "expectation": "expect_column_values_to_be_between",
            "column": column,
            "success": success,
            "min": min_val,
            "max": max_val,
            "in_range_ratio": round(ratio, 4)
        })
        return self
    
    def expect_table_row_count_to_be_between(self, min_rows: int, max_rows: int) -> "DataQualityValidator":
        """Expect table to have between min and max rows."""
        row_count = len(self.df)
        success = min_rows <= row_count <= max_rows
        self.results.append({
            "expectation": "expect_table_row_count_to_be_between",
            "success": success,
            "row_count": row_count,
            "min": min_rows,
            "max": max_rows
        })
        if not success:
            self.passed = False
        return self
    
    def validate(self) -> dict:
        """Run all validations and return results."""
        passed_count = sum(1 for r in self.results if r["success"])
        failed_count = len(self.results) - passed_count
        
        return {
            "success": self.passed,
            "statistics": {
                "evaluated_expectations": len(self.results),
                "successful_expectations": passed_count,
                "unsuccessful_expectations": failed_count
            },
            "results": self.results
        }


def validate_gdelt_data(df: pl.DataFrame) -> dict:
    """
    Run data quality checks on GDELT data.
    Returns validation results dict.
    """
    validator = DataQualityValidator(df)
    
    # Required columns must exist
    validator.expect_column_to_exist("EVENT_ID")
    validator.expect_column_to_exist("DATE")
    validator.expect_column_to_exist("MAIN_ACTOR")
    
    # Primary key should not be null
    validator.expect_column_values_to_not_be_null("EVENT_ID", threshold=1.0)
    validator.expect_column_values_to_not_be_null("DATE", threshold=1.0)
    
    # Check for reasonable row count (GDELT 15-min batch)
    validator.expect_table_row_count_to_be_between(100, 50000)
    
    # Goldstein scale should be between -10 and 10
    validator.expect_column_values_to_be_between("IMPACT_SCORE", -10.0, 10.0)
    
    # Sentiment score (AvgTone) typically between -100 and 100
    validator.expect_column_values_to_be_between("SENTIMENT_SCORE", -100.0, 100.0)
    
    return validator.validate()


# =============================================================================
# HEADLINE EXTRACTION (Same logic, cleaner code)
# =============================================================================

def extract_headline_from_url(url: str) -> str | None:
    """Extract clean headline from URL at ingestion time."""
    if not url or not isinstance(url, str):
        return None
    
    try:
        parsed = urlparse(url)
        path = unquote(parsed.path)
        segments = [s for s in path.split('/') if s and len(s) > 8]
        if not segments:
            return None
        
        for seg in reversed(segments):
            headline = clean_url_segment(seg)
            if headline and len(headline) > 20:
                return headline
        return None
    except Exception:
        return None


def clean_url_segment(text: str) -> str | None:
    """Clean a URL path segment into a readable headline."""
    if not text:
        return None
    
    text = str(text).strip()
    
    # Reject garbage patterns
    reject_patterns = [
        r'^[a-f0-9]{8}[-][a-f0-9]{4}',
        r'^[a-f0-9\-]{20,}$',
        r'^(article|post|item|id)[-_]?[a-f0-9]{6,}',
        r'^\d{10,}$',
        r'^\d+$',
        r'^[A-Z]{2,5}\s*\d{5,}',
    ]
    
    for pattern in reject_patterns:
        if re.match(pattern, text.lower()):
            return None
    
    # Clean up text
    text = re.sub(r'\.(html?|php|aspx?|jsp|shtml)$', '', text, flags=re.I)
    text = re.sub(r'^\d{8}[-_]?', '', text)
    text = re.sub(r'^\d{4}[-/]\d{2}[-/]\d{2}[-_]?', '', text)
    text = re.sub(r'^\d{4}[-_]', '', text)
    text = re.sub(r'^[a-f0-9]{6,8}[-_]', '', text)
    text = re.sub(r'[-_]+', ' ', text)
    text = re.sub(r'\s+\d{5,}$', '', text)
    text = re.sub(r'\s+[a-f0-9]{8,}$', '', text, flags=re.I)
    text = re.sub(r'\s+\d{1,8}$', '', text)
    text = ' '.join(text.split())
    text = re.sub(r'^[.,;:\'\"!?\-_\s]+', '', text)
    text = re.sub(r'[.,;:\'\"!?\-_\s]+$', '', text)
    
    # Validate
    if len(text) < 15 or ' ' not in text:
        return None
    
    words = text.split()
    if len(words) < 3:
        return None
    
    # Truncate and title case
    if len(text) > 100:
        text = text[:100].rsplit(' ', 1)[0]
    
    if len(text) < 15:
        return None
    
    # Title case
    text = text.lower()
    words = text.split()
    small_words = {'a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'on', 'at', 
                   'to', 'by', 'in', 'of', 'up', 'as', 'is', 'it', 'so', 'be'}
    result = []
    for i, word in enumerate(words):
        if i == 0:
            result.append(word.capitalize())
        elif word in small_words:
            result.append(word)
        else:
            result.append(word.capitalize())
    
    return ' '.join(result)


# =============================================================================
# POLARS-BASED DATA PROCESSING
# =============================================================================

def get_gdelt_url() -> str:
    """Get the URL for the latest GDELT export file."""
    now = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(minutes=20)
    rounded_minute = (now.minute // 15) * 15
    rounded_time = now.replace(minute=rounded_minute, second=0, microsecond=0)
    timestamp = rounded_time.strftime("%Y%m%d%H%M00")
    return f"http://data.gdeltproject.org/gdeltv2/{timestamp}.export.CSV.zip"


def download_with_retry(url: str, max_retries: int = MAX_RETRIES, timeout: int = 30):
    """Download with exponential backoff retry."""
    for attempt in range(max_retries):
        try:
            logger.info(f"Download attempt {attempt + 1}/{max_retries}: {url}")
            response = requests.get(url, timeout=timeout)
            if response.status_code == 200:
                logger.info(f"Download successful ({len(response.content)} bytes)")
                return response
            elif response.status_code == 404:
                logger.warning("File not found (404)")
                return None
        except requests.exceptions.Timeout:
            logger.warning(f"Timeout on attempt {attempt + 1}")
        except requests.exceptions.RequestException as e:
            logger.warning(f"Network error: {e}")
        
        if attempt < max_retries - 1:
            time.sleep(RETRY_DELAY * (2 ** attempt))
    
    logger.error(f"Failed after {max_retries} attempts")
    return None


def process_gdelt_batch_polars(content: bytes) -> pl.DataFrame:
    """
    Process GDELT CSV content using Polars.
    10x faster than Pandas for this operation.
    """
    z = zipfile.ZipFile(io.BytesIO(content))
    csv_name = z.namelist()[0]
    
    with z.open(csv_name) as f:
        # Read with Polars - significantly faster than Pandas
        df = pl.read_csv(
            f,
            separator='\t',
            has_header=False,
            columns=[0, 1, 6, 7, 29, 30, 31, 34, 60],  # Select only needed columns
            new_columns=["EVENT_ID", "DATE", "MAIN_ACTOR", "ACTOR_COUNTRY_CODE", 
                        "EVENT_CATEGORY_CODE", "IMPACT_SCORE", "ARTICLE_COUNT", 
                        "SENTIMENT_SCORE", "NEWS_LINK"],
            infer_schema_length=10000,
            ignore_errors=True,
        )
    
    # Cast columns to correct types
    df = df.with_columns([
        pl.col("EVENT_ID").cast(pl.Utf8),
        pl.col("DATE").cast(pl.Utf8),
        pl.col("IMPACT_SCORE").cast(pl.Float64),
        pl.col("ARTICLE_COUNT").cast(pl.Float64),
        pl.col("SENTIMENT_SCORE").cast(pl.Float64),
    ])
    
    return df


def select_best_headline_per_event_polars(df: pl.DataFrame) -> pl.DataFrame:
    """
    Group by EVENT_ID, extract best headline from URLs.
    Polars implementation with lazy evaluation.
    """
    if df.is_empty():
        return df.with_columns(pl.lit(None).alias("HEADLINE"))
    
    logger.info(f"Selecting best headlines from {len(df):,} rows...")
    
    # Add headline column by extracting from URLs
    # We need to do this row by row since headline extraction is complex
    headlines = []
    for url in df["NEWS_LINK"].to_list():
        headlines.append(extract_headline_from_url(url))
    
    df = df.with_columns(pl.Series("HEADLINE", headlines))
    
    # Group by EVENT_ID and keep row with best headline
    # Use Polars' powerful groupby + agg
    df = df.with_columns(
        pl.col("HEADLINE").is_not_null().alias("has_headline"),
        pl.col("HEADLINE").str.len_chars().fill_null(0).alias("headline_len")
    )
    
    # Sort to prioritize rows with headlines, then by headline length
    df = df.sort(["EVENT_ID", "has_headline", "headline_len"], descending=[False, True, True])
    
    # Keep first (best) row per EVENT_ID
    df = df.group_by("EVENT_ID", maintain_order=True).first()
    
    # Count articles per event (our own count from duplicate URLs)
    article_counts = df.group_by("EVENT_ID").len().rename({"len": "OUR_ARTICLE_COUNT"})
    df = df.join(article_counts, on="EVENT_ID", how="left")
    
    # Use our article count if higher than GDELT's
    df = df.with_columns(
        pl.when(pl.col("OUR_ARTICLE_COUNT") > pl.col("ARTICLE_COUNT"))
        .then(pl.col("OUR_ARTICLE_COUNT"))
        .otherwise(pl.col("ARTICLE_COUNT"))
        .alias("ARTICLE_COUNT")
    )
    
    # Drop temp columns
    df = df.drop(["has_headline", "headline_len", "OUR_ARTICLE_COUNT"])
    
    headlines_found = df.filter(pl.col("HEADLINE").is_not_null()).height
    logger.info(f"Selected {len(df):,} events with {headlines_found:,} headlines")
    
    return df


# =============================================================================
# DAGSTER ASSETS - INGESTION JOB (Every 15 minutes)
# =============================================================================

@asset(description="Extract and transform GDELT data using Polars (no embeddings)")
def gdelt_raw_data_polars(context: AssetExecutionContext) -> pl.DataFrame:
    """
    Extract raw GDELT data with Polars.
    This job runs every 15 minutes and does NOT compute embeddings.
    Embeddings are computed by a separate job every 12 hours.
    """
    logger.info("üöÄ Starting GDELT extraction (Polars-powered)")
    url = get_gdelt_url()
    
    response = download_with_retry(url)
    if not response:
        context.log.warning("Download failed, returning empty DataFrame")
        return pl.DataFrame()
    
    try:
        # Process with Polars (10x faster than Pandas)
        df = process_gdelt_batch_polars(response.content)
        logger.info(f"üìä Loaded {len(df):,} rows with Polars")
        
        # Data Quality Validation (Great Expectations style)
        logger.info("üîç Running data quality validation...")
        validation_result = validate_gdelt_data(df)
        
        if not validation_result["success"]:
            context.log.warning(f"‚ö†Ô∏è Data quality issues: {validation_result['results']}")
            # Log but don't fail - we'll filter bad data
        else:
            context.log.info(f"‚úÖ Data quality passed: {validation_result['statistics']}")
        
        # Filter out rows with null EVENT_ID
        df = df.filter(pl.col("EVENT_ID").is_not_null())
        
        # Select best headline per event
        df = select_best_headline_per_event_polars(df)
        
        # Add EMBEDDING column as null (will be populated by separate job)
        df = df.with_columns(pl.lit(None).cast(pl.List(pl.Float64)).alias("EMBEDDING"))
        
        logger.info(f"‚úÖ Processed {len(df):,} unique events")
        return df
        
    except Exception as e:
        logger.error(f"‚ùå Error processing GDELT data: {e}")
        context.log.error(f"Processing error: {e}")
        return pl.DataFrame()


@asset(description="Load data into MotherDuck with deduplication")
def gdelt_motherduck_table_polars(context: AssetExecutionContext, gdelt_raw_data_polars: pl.DataFrame) -> Output:
    """Load Polars DataFrame into MotherDuck with deduplication."""
    if gdelt_raw_data_polars.is_empty():
        return Output(None, metadata={"status": "Skipped", "rows": 0})

    token = os.getenv("MOTHERDUCK_TOKEN")
    if not token:
        return Output(None, metadata={"status": "Error", "message": "Missing token"})
    
    try:
        # Convert Polars to Pandas for DuckDB insertion
        # (DuckDB has better Pandas support currently)
        pdf = gdelt_raw_data_polars.to_pandas()
        
        with duckdb.connect(f'md:gdelt_db?motherduck_token={token}') as con:
            try:
                # Check if EMBEDDING column exists
                try:
                    con.execute(f"SELECT EMBEDDING FROM {TARGET_TABLE} LIMIT 1")
                except:
                    logger.info("Adding EMBEDDING column to table...")
                    con.execute(f"ALTER TABLE {TARGET_TABLE} ADD COLUMN EMBEDDING DOUBLE[]")
                
                # Deduplicate against existing data
                existing = con.execute(f"""
                    SELECT EVENT_ID FROM {TARGET_TABLE} 
                    WHERE DATE >= '{pdf['DATE'].min()}'
                """).df()
                
                if not existing.empty:
                    pdf = pdf[~pdf['EVENT_ID'].isin(existing['EVENT_ID'])]
                
                if pdf.empty:
                    return Output("No new data", metadata={"status": "Skipped", "rows": 0})
                
                # Register and insert
                con.register('new_data', pdf)
                con.execute(f"""
                    INSERT INTO {TARGET_TABLE} 
                    (EVENT_ID, DATE, MAIN_ACTOR, ACTOR_COUNTRY_CODE, EVENT_CATEGORY_CODE, 
                     IMPACT_SCORE, ARTICLE_COUNT, SENTIMENT_SCORE, NEWS_LINK, HEADLINE, EMBEDDING)
                    SELECT EVENT_ID, DATE, MAIN_ACTOR, ACTOR_COUNTRY_CODE, EVENT_CATEGORY_CODE, 
                           IMPACT_SCORE, ARTICLE_COUNT, SENTIMENT_SCORE, NEWS_LINK, HEADLINE, EMBEDDING
                    FROM new_data
                """)
                
                total = con.execute(f"SELECT COUNT(*) FROM {TARGET_TABLE}").fetchone()[0]
                embedded = con.execute(f"SELECT COUNT(*) FROM {TARGET_TABLE} WHERE EMBEDDING IS NOT NULL").fetchone()[0]
                
                context.log.info(f"‚úÖ Inserted {len(pdf):,} rows. Total: {total:,}, Embedded: {embedded:,}")
                return Output(
                    f"Inserted {len(pdf):,}", 
                    metadata={"rows": len(pdf), "total": total, "embedded": embedded}
                )
                
            except Exception as e:
                if "does not exist" in str(e).lower():
                    con.register('new_data', pdf)
                    con.execute(f"CREATE TABLE {TARGET_TABLE} AS SELECT * FROM new_data")
                    return Output(f"Created table with {len(pdf):,} rows", metadata={"rows": len(pdf)})
                else:
                    raise e
                
    except Exception as e:
        context.log.error(f"‚ùå MotherDuck error: {e}")
        return Output(None, metadata={"status": "Error", "message": str(e)})


# =============================================================================
# GKG (GLOBAL KNOWLEDGE GRAPH) - EMOTIONS & THEMES
# =============================================================================

GKG_TABLE = "gkg_emotions"

# Key GCAM emotion codes we want to extract
# Format: c{dictionary_id}.{dimension_id}
GCAM_EMOTIONS = {
    "c9.1": "fear",
    "c9.2": "anger", 
    "c9.3": "sadness",
    "c9.4": "joy",
    "c9.5": "disgust",
    "c9.6": "surprise",
    "c9.7": "trust",
    "c9.8": "anticipation",
    "c18.1": "anxiety",
    "c18.2": "hostility",
    "c18.3": "depression",
}


def get_gdelt_gkg_url() -> str:
    """Get the URL for the latest GDELT GKG file."""
    now = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(minutes=20)
    rounded_minute = (now.minute // 15) * 15
    rounded_time = now.replace(minute=rounded_minute, second=0, microsecond=0)
    timestamp = rounded_time.strftime("%Y%m%d%H%M00")
    return f"http://data.gdeltproject.org/gdeltv2/{timestamp}.gkg.csv.zip"


def parse_gcam_field(gcam_str: str) -> dict:
    """
    Parse GCAM field into emotion scores.
    GCAM format: dimension:value,dimension:value,...
    """
    emotions = {name: 0.0 for name in GCAM_EMOTIONS.values()}
    
    if not gcam_str or not isinstance(gcam_str, str):
        return emotions
    
    try:
        for pair in gcam_str.split(","):
            if ":" in pair:
                parts = pair.split(":")
                if len(parts) == 2:
                    code, value = parts
                    if code in GCAM_EMOTIONS:
                        try:
                            emotions[GCAM_EMOTIONS[code]] = float(value)
                        except ValueError:
                            pass
    except Exception:
        pass
    
    return emotions


def parse_themes_field(themes_str: str) -> list:
    """Extract top themes from GKG themes field."""
    if not themes_str or not isinstance(themes_str, str):
        return []
    
    try:
        # Themes are semicolon-separated, may have character offsets
        themes = []
        for item in themes_str.split(";"):
            # Remove character offset if present (format: THEME,offset)
            theme = item.split(",")[0].strip()
            if theme and len(theme) > 2:
                themes.append(theme)
        return themes[:10]  # Keep top 10 themes
    except Exception:
        return []


def process_gkg_batch_polars(content: bytes) -> pl.DataFrame:
    """
    Process GDELT GKG CSV content using Polars.
    Extracts emotions and themes from news articles.
    """
    z = zipfile.ZipFile(io.BytesIO(content))
    csv_name = z.namelist()[0]
    
    with z.open(csv_name) as f:
        # GKG columns we need:
        # 0: GKGRECORDID, 1: DATE, 3: SourceCommonName, 7: Themes
        # 11: Persons, 12: Organizations, 15: Tone, 17: GCAM
        try:
            df = pl.read_csv(
                f,
                separator='\t',
                has_header=False,
                columns=[0, 1, 3, 7, 11, 12, 15, 17],
                new_columns=["GKG_ID", "DATE", "SOURCE", "THEMES", 
                            "PERSONS", "ORGS", "TONE", "GCAM"],
                infer_schema_length=10000,
                ignore_errors=True,
                truncate_ragged_lines=True,
            )
        except Exception as e:
            logger.error(f"Error reading GKG CSV: {e}")
            return pl.DataFrame()
    
    if df.is_empty():
        return df
    
    # Parse TONE field (format: tone,positive,negative,polarity,activity,self/group)
    def extract_tone(tone_str):
        if not tone_str:
            return (0.0, 0.0, 0.0)
        try:
            parts = str(tone_str).split(",")
            avg_tone = float(parts[0]) if len(parts) > 0 else 0.0
            positive = float(parts[1]) if len(parts) > 1 else 0.0
            negative = float(parts[2]) if len(parts) > 2 else 0.0
            return (avg_tone, positive, negative)
        except:
            return (0.0, 0.0, 0.0)
    
    # Extract tone components
    tones = [extract_tone(t) for t in df["TONE"].to_list()]
    df = df.with_columns([
        pl.Series("AVG_TONE", [t[0] for t in tones]),
        pl.Series("POSITIVE_SCORE", [t[1] for t in tones]),
        pl.Series("NEGATIVE_SCORE", [t[2] for t in tones]),
    ])
    
    # Parse GCAM emotions
    gcam_data = [parse_gcam_field(g) for g in df["GCAM"].to_list()]
    for emotion_name in GCAM_EMOTIONS.values():
        df = df.with_columns(
            pl.Series(f"EMOTION_{emotion_name.upper()}", 
                     [d.get(emotion_name, 0.0) for d in gcam_data])
        )
    
    # Parse themes into list (store as comma-separated string for simplicity)
    themes_list = [",".join(parse_themes_field(t)) for t in df["THEMES"].to_list()]
    df = df.with_columns(pl.Series("TOP_THEMES", themes_list))
    
    # Clean up - drop raw fields, keep processed ones
    df = df.drop(["TONE", "GCAM", "THEMES"])
    
    # Filter out rows with null GKG_ID
    df = df.filter(pl.col("GKG_ID").is_not_null())
    
    logger.info(f"üìä Processed {len(df):,} GKG records")
    return df


@asset(description="Extract emotions and themes from GDELT GKG")
def gdelt_gkg_data(context: AssetExecutionContext) -> pl.DataFrame:
    """
    Extract GKG data with emotions and themes.
    Runs alongside event ingestion every 15 minutes.
    """
    logger.info("üß† Starting GDELT GKG extraction (emotions & themes)")
    url = get_gdelt_gkg_url()
    
    response = download_with_retry(url)
    if not response:
        context.log.warning("GKG download failed, returning empty DataFrame")
        return pl.DataFrame()
    
    try:
        df = process_gkg_batch_polars(response.content)
        if df.is_empty():
            context.log.warning("No GKG data processed")
            return df
        
        logger.info(f"‚úÖ Extracted {len(df):,} GKG records with emotions")
        return df
        
    except Exception as e:
        logger.error(f"‚ùå Error processing GKG data: {e}")
        context.log.error(f"GKG processing error: {e}")
        return pl.DataFrame()


@asset(description="Load GKG emotions data into MotherDuck")
def gdelt_gkg_motherduck(context: AssetExecutionContext, gdelt_gkg_data: pl.DataFrame) -> Output:
    """Load GKG emotions data into MotherDuck."""
    if gdelt_gkg_data.is_empty():
        return Output(None, metadata={"status": "Skipped", "rows": 0})
    
    token = os.getenv("MOTHERDUCK_TOKEN")
    if not token:
        return Output(None, metadata={"status": "Error", "message": "Missing token"})
    
    try:
        pdf = gdelt_gkg_data.to_pandas()
        
        with duckdb.connect(f'md:gdelt_db?motherduck_token={token}') as con:
            try:
                # Check if table exists and deduplicate
                existing = con.execute(f"""
                    SELECT GKG_ID FROM {GKG_TABLE} 
                    WHERE DATE >= '{pdf['DATE'].min()}'
                """).df()
                
                if not existing.empty:
                    pdf = pdf[~pdf['GKG_ID'].isin(existing['GKG_ID'])]
                
                if pdf.empty:
                    return Output("No new GKG data", metadata={"status": "Skipped", "rows": 0})
                
                con.register('gkg_data', pdf)
                con.execute(f"INSERT INTO {GKG_TABLE} SELECT * FROM gkg_data")
                
                total = con.execute(f"SELECT COUNT(*) FROM {GKG_TABLE}").fetchone()[0]
                context.log.info(f"‚úÖ Inserted {len(pdf):,} GKG rows. Total: {total:,}")
                return Output(f"Inserted {len(pdf):,}", metadata={"rows": len(pdf), "total": total})
                
            except Exception as e:
                if "does not exist" in str(e).lower():
                    con.register('gkg_data', pdf)
                    con.execute(f"CREATE TABLE {GKG_TABLE} AS SELECT * FROM gkg_data")
                    context.log.info(f"‚úÖ Created {GKG_TABLE} with {len(pdf):,} rows")
                    return Output(f"Created table with {len(pdf):,} rows", metadata={"rows": len(pdf)})
                else:
                    raise e
                
    except Exception as e:
        context.log.error(f"‚ùå GKG MotherDuck error: {e}")
        return Output(None, metadata={"status": "Error", "message": str(e)})


# =============================================================================
# DAGSTER JOBS & SCHEDULES
# =============================================================================

# Main ingestion job - runs every 15 minutes (now includes GKG)
gdelt_ingestion_job = define_asset_job(
    name="gdelt_ingestion_job",
    selection=["gdelt_raw_data_polars", "gdelt_motherduck_table_polars", 
               "gdelt_gkg_data", "gdelt_gkg_motherduck"],
    description="Ingest GDELT events + GKG emotions every 15 minutes"
)

# Schedule: Every 15 minutes
gdelt_ingestion_schedule = ScheduleDefinition(
    job=gdelt_ingestion_job,
    cron_schedule="*/15 * * * *",  # Every 15 minutes
    execution_timezone="UTC",
    description="Run GDELT ingestion every 15 minutes"
)

# Definitions
defs = Definitions(
    assets=[gdelt_raw_data_polars, gdelt_motherduck_table_polars,
            gdelt_gkg_data, gdelt_gkg_motherduck],
    jobs=[gdelt_ingestion_job],
    schedules=[gdelt_ingestion_schedule]
)

